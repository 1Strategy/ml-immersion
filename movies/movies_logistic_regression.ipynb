{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Strategy ML Immersion Day\n",
    "### Building a model from movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker as sm\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import workshop_utils as wu\n",
    "\n",
    "# prevent warnings from displaying\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket    = '1s-ml'\n",
    "your_name = 'agraves'\n",
    "\n",
    "model_artifacts_location = f's3://{bucket}/movies/artifacts/{your_name}'\n",
    "\n",
    "role = sm.get_execution_role()\n",
    "sm_session = sm.session.Session()\n",
    "\n",
    "print(f'IAM Role: {role}')\n",
    "\n",
    "ratings = 'movies/data/title.ratings.tsv'\n",
    "basics = 'movies/data/title.basics.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about this data\n",
    "source: https://datasets.imdbws.com\n",
    "\n",
    "We will be downloading the data from S3 in order to inspect it and perform any cleanup necessary before we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file(ratings, 'ratings.tsv')\n",
    "s3.Bucket(bucket).download_file(basics, 'basics.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_csv = pd.read_csv('ratings.tsv', sep='\\t')\n",
    "basics_csv = pd.read_csv('basics.tsv', sep='\\t')\n",
    "movie_data = pd.merge(ratings_csv, basics_csv, how='inner', on='tconst')\n",
    "print(f'Movie Data Shape: {movie_data.shape}')\n",
    "\n",
    "movie_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "There are several unecessary columns in this data as well as observations we aren't concerned about. This is an investigation of movie ratings, so we can eliminate the rows which contain data about television shows. This data also contains records from silent films. We can make a reasonable assumption that silent film appreciation is a bit different than modern film appreciation, so we will drop these observations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate TV Shows\n",
    "movie_data = movie_data[(movie_data.titleType == 'movie') | (movie_data.titleType == 'short') | (movie_data.titleType == 'tvMovie')]\n",
    "# Shape: (395863, 11)\n",
    "\n",
    "# Limit to only years with talkies\n",
    "movie_data = movie_data[movie_data.startYear != '\\\\N']\n",
    "movie_data.startYear = movie_data.startYear.astype(int)\n",
    "movie_data = movie_data[movie_data.startYear > 1927]\n",
    "# Shape: (383612, 11)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "movie_data.drop('originalTitle', axis=1, inplace=True)\n",
    "movie_data.drop('endYear', axis=1, inplace=True)\n",
    "movie_data.drop('tconst', axis=1, inplace=True)\n",
    "movie_data.drop('primaryTitle', axis=1, inplace=True)\n",
    "movie_data.drop('genres', axis=1, inplace=True)\n",
    "# I am working to one hot encode the genres column. It requires a custom function.\n",
    "movie_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \\\\N to NaN\n",
    "movie_data = movie_data[movie_data != r'\\N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see how many NaN values we have now that we've dropped the /N entries\n",
    "movie_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any observations with null values\n",
    "movie_data.dropna(inplace=True)\n",
    "movie_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a figure instance\n",
    "# fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# # Create an axes instance\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# # Create the boxplot\n",
    "# bp = ax.boxplot([movie_data[:100].runtimeMinutes])\n",
    "\n",
    "plt.plot(movie_data.titleType, movie_data.numVotes, 'o')\n",
    "\n",
    "# # Save the figure\n",
    "# fig.savefig('fig1.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(movie_data.runtimeMinutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likable = movie_data.apply(lambda row: wu.label_rating(row), axis=1)\n",
    "movie_data = pd.concat([likable, movie_data], axis=1)\n",
    "movie_data.rename(columns={0:'likable'}, inplace=True)\n",
    "movie_data.drop('averageRating', axis=1, inplace=True)\n",
    "movie_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode titleType column\n",
    "dummy_types = pd.get_dummies(movie_data['titleType'])\n",
    "movie_data = pd.concat([movie_data, dummy_types.reindex(movie_data.index)], axis=1)\n",
    "movie_data.drop('titleType', axis=1, inplace=True)\n",
    "\n",
    "movie_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have only numbers in runtimeMinutes, we can convert to int\n",
    "# movie_data['runtimeMinutes'] = movie_data['runtimeMinutes'].astype(int)\n",
    "# movie_data['likable'] = movie_data['likable'].astype(int)\n",
    "movie_data['movie'] = movie_data['movie'].astype(int)\n",
    "movie_data['short'] = movie_data['short'].astype(int)\n",
    "movie_data['tvMovie'] = movie_data['tvMovie'].astype(int)\n",
    "movie_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train, movie_eval, movie_test = np.split(movie_data.sample(frac=1, random_state=1278), [int(0.7 * len(movie_data)), int(0.9 * len(movie_data))])\n",
    "print(f'Movie Train Shape: {movie_train.shape}')\n",
    "print(f'Movie Eval Shape: {movie_eval.shape}')\n",
    "print(f'Movie Test Shape: {movie_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train.to_csv('movie_train.csv', header=False, index=False)\n",
    "train_upload = f'movies/artifacts/{your_name}/movie_train.csv'\n",
    "print(train_upload)\n",
    "s3.Bucket(bucket).Object(train_upload).upload_file('movie_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_eval.to_csv('movie_eval.csv', header=False, index=False)\n",
    "eval_upload = f'movies/artifacts/{your_name}/movie_eval.csv'\n",
    "print(train_upload)\n",
    "s3.Bucket(bucket).Object(eval_upload).upload_file('movie_eval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ML resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri('us-west-2', 'xgboost', '0.90-1')\n",
    "\n",
    "xgboost = sm.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    base_job_name=f'{your_name}-ml-im',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    output_path=f's3://{bucket}/movies/artifacts/{your_name}/output',\n",
    "    sagemaker_session=sm_session)\n",
    "# eval_metric='auc',\n",
    "# objective='reg:linear',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.set_hyperparameters(\n",
    "    max_depth=3,\n",
    "    eta=0.1,\n",
    "    subsample=0.5,\n",
    "    eval_metric='error',\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=2.0,\n",
    "    num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sm.s3_input(s3_data=f's3://{bucket}/{train_upload}', content_type='csv')\n",
    "eval_data = sm.s3_input(s3_data=f's3://{bucket}/{eval_upload}', content_type='csv')\n",
    "\n",
    "xgboost.fit({'train': train_data, 'validation': eval_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predict = xgboost.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large')\n",
    "xgboost_predict.content_type = 'text/csv'\n",
    "xgboost_predict.deserializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_test.drop('likable', axis=1, inplace=True)\n",
    "movie_test.to_csv('movie_test.csv', header=False, index=False)\n",
    "movie_test.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie_test.csv', 'r') as file:\n",
    "    payload = file.read().strip()\n",
    "\n",
    "test_data = [line for line in payload.split('\\n')]\n",
    "preds = wu.do_predict(test_data, xgboost_predict)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session.delete_endpoint(xgboost_predict.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
